{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing"
      ],
      "metadata": {
        "id": "G1AV--VHx-Rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "cdWy7-M0yFYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "\n",
        "import spacy # import the Spacy library\n",
        "\n",
        "# Load language library\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "HWJ2VbPB4zWT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mystring = \"'We\\'re moving to L.A.!'\"\n",
        "print(mystring)"
      ],
      "metadata": {
        "id": "HjBJO--L5Bod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da44e184-557f-412d-d8f3-3294aca386fd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'We're moving to L.A.!'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(mystring)\n",
        "\n",
        "for token in doc:\n",
        "\n",
        "  print(token.text, end = ' | ')\n",
        "\n",
        "\n",
        "# Prefixes, Suffixes and Infixes\n",
        "# spaCy will isolate punctuation that does *not* form an integral part of a word.\n",
        "\n",
        "# Quotation marks, commas, and punctuation at the end of a sentence will be assigned their own token.\n",
        "# However, punctuation that exists as part of an email address, website or numerical value will be kept as part of the token."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjBal75BEo-b",
        "outputId": "edab7a98-e0e2-4f1c-fe1b-1f0f2933ea80"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "' | We | 're | moving | to | L.A. | ! | ' | "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(u\"We're here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!\")\n",
        "\n",
        "for t in doc2:\n",
        "\n",
        "    print(t, end = ' | ')\n",
        "\n",
        "# Note that the exclamation points, comma, and the hyphen in 'snail-mail' are assigned their own tokens,\n",
        "# yet both the email address and website are preserved."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV9Rw23lE0Hq",
        "outputId": "08365d06-e0f6-4948-b1b9-edc515c2b309"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We | 're | here | to | help | ! | Send | snail | - | mail | , | email | support@oursite.com | or | visit | us | at | http://www.oursite.com | ! | "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp(u'A 5km NYC cab ride costs $10.30')\n",
        "\n",
        "for t in doc3:\n",
        "\n",
        "    print(t)\n",
        "\n",
        "# Here the distance unit and dollar sign are assigned their own tokens,\n",
        "# yet the dollar amount is preserved."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwKEmwfPFwYK",
        "outputId": "7b8d4389-d69f-4f93-eab1-652222b33283"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "5\n",
            "km\n",
            "NYC\n",
            "cab\n",
            "ride\n",
            "costs\n",
            "$\n",
            "10.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc4 = nlp(u\"Let's visit St. Louis in the U.S. next year.\")\n",
        "\n",
        "for t in doc4:\n",
        "\n",
        "    print(t)\n",
        "\n",
        "# Here the abbreviations for \"Saint\" and \"United States\" are both preserved."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7aFvNHQF_vL",
        "outputId": "3cbabfea-5360-4834-c615-0bffcf8c9d69"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let\n",
            "'s\n",
            "visit\n",
            "St.\n",
            "Louis\n",
            "in\n",
            "the\n",
            "U.S.\n",
            "next\n",
            "year\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Counting Tokens"
      ],
      "metadata": {
        "id": "MsdV1GifGfix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjcpnkARGgtl",
        "outputId": "4bd03d84-34a7-4873-afa4-d2412a1bc569"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Counting Vocal Entries"
      ],
      "metadata": {
        "id": "A4-qttZ2G2bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc4.vocab)\n",
        "\n",
        "# This number changes based on the language library loaded at the start, and any\n",
        "# new lexemes introduced to the `vocab` when the `Doc` was created"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKlDT997G4iU",
        "outputId": "b27d7664-f242-4e31-8a1a-21061c41458b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "794"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokens cannot be re-assigned"
      ],
      "metadata": {
        "id": "e-dGiPFNHaLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc6 = nlp(u'My dinner was horrible.')\n",
        "doc7 = nlp(u'Your dinner was delicious.')\n",
        "\n",
        "# Try to change \"My dinner was horrible\" to \"My dinner was delicious\"\n",
        "\n",
        "doc6[3] = doc7[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "jmgxiTPKHeRW",
        "outputId": "1b6e8daa-af69-4a21-a9cf-07c3d33ba01f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'spacy.tokens.doc.Doc' object does not support item assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-2f1a36dd26de>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Try to change \"My dinner was horrible\" to \"My dinner was delicious\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdoc6\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc7\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'spacy.tokens.doc.Doc' object does not support item assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entities\n",
        "\n",
        "Going a step beyond tokens, *named entities* add another layer of context. The language model recognizes that certain words are organizational names while others are locations, and still other combinations relate to money, dates, etc. Named entities are accessible through the `ents` property of a `Doc` object."
      ],
      "metadata": {
        "id": "lBUXt92qHvUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc8 = nlp(u'Apple to build a Hong Kong factory for $6 million')\n",
        "\n",
        "for token in doc8:\n",
        "\n",
        "    print(token.text, end = ' | ')\n",
        "\n",
        "print('\\n----')\n",
        "\n",
        "for ent in doc8.ents:\n",
        "\n",
        "    print(ent.text + ' - '+ ent.label_ + ' - ' + str(spacy.explain(ent.label_)))\n",
        "\n",
        "# Note how two tokens combine to form the entity `Hong Kong`,\n",
        "# and three tokens combine to form the monetary entity:  `$6 million`\n",
        "\n",
        "# Named Entity Recognition (NER) is an important machine learning tool\n",
        "# applied to Natural Language Processing."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYkochlkHwwd",
        "outputId": "378b96b7-82bb-4026-b8de-3985abc78c80"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple | to | build | a | Hong | Kong | factory | for | $ | 6 | million | \n",
            "----\n",
            "Apple - ORG - Companies, agencies, institutions, etc.\n",
            "Hong Kong - GPE - Countries, cities, states\n",
            "$6 million - MONEY - Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Noun Chunks\n",
        "\n",
        "Similar to `Doc.ents`, `Doc.noun_chunks` are another object property. *Noun chunks* are \"base noun phrases\" – flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun – for example, in [Sheb Wooley's 1958 song](https://en.wikipedia.org/wiki/The_Purple_People_Eater), a *\"one-eyed, one-horned, flying, purple people-eater\"* would be one long noun chunk."
      ],
      "metadata": {
        "id": "Yq8JuyEmH4i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc9 = nlp(u\"Autonomous cars shift insurance liability toward manufacturers.\")\n",
        "\n",
        "for chunk in doc9.noun_chunks:\n",
        "\n",
        "    print(chunk.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyWH8rHOJBji",
        "outputId": "8fcf23e9-fc9b-40b9-d232-6f685ecc9922"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autonomous cars\n",
            "insurance liability\n",
            "manufacturers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc10 = nlp(u\"Red cars do not carry higher insurance rates.\")\n",
        "\n",
        "for chunk in doc10.noun_chunks:\n",
        "    print(chunk.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERDwmN7XJRB1",
        "outputId": "91362a42-38ec-4263-c3c8-0e3d6a2957a1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Red cars\n",
            "higher insurance rates\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc11 = nlp(u\"He was a one-eyed, one-horned, flying, purple people-eater.\")\n",
        "\n",
        "for chunk in doc11.noun_chunks:\n",
        "\n",
        "    print(chunk.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeLAd1z7JYpD",
        "outputId": "0a8b9cfc-b91d-48dc-8dbf-2c1b4553ab14"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He\n",
            "a one-eyed, one-horned, flying, purple people-eater\n"
          ]
        }
      ]
    }
  ]
}